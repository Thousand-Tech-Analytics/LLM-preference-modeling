{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Preference Modeling Baseline (Chatbot Arena)\n",
    "\n",
    "This notebook builds a reproducible baseline for **human preference prediction** given a prompt and two candidate model responses.\n",
    "We model the outcome as a **3-class classification** problem: `A wins`, `B wins`, or `tie` (metric: **multiclass log loss**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:26:35.727921Z",
     "iopub.status.busy": "2026-02-11T03:26:35.726869Z",
     "iopub.status.idle": "2026-02-11T03:26:35.733826Z",
     "shell.execute_reply": "2026-02-11T03:26:35.732530Z",
     "shell.execute_reply.started": "2026-02-11T03:26:35.727876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-11T03:26:52.396968Z",
     "iopub.status.busy": "2026-02-11T03:26:52.396232Z",
     "iopub.status.idle": "2026-02-11T03:26:52.415938Z",
     "shell.execute_reply": "2026-02-11T03:26:52.414565Z",
     "shell.execute_reply.started": "2026-02-11T03:26:52.396929Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/llm-classification-finetuning/sample_submission.csv\n",
      "/kaggle/input/llm-classification-finetuning/train.csv\n",
      "/kaggle/input/llm-classification-finetuning/test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:27:04.211053Z",
     "iopub.status.busy": "2026-02-11T03:27:04.210117Z",
     "iopub.status.idle": "2026-02-11T03:27:07.124891Z",
     "shell.execute_reply": "2026-02-11T03:27:07.124156Z",
     "shell.execute_reply.started": "2026-02-11T03:27:04.211016Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (57477, 9)\n",
      "test : (3, 4)\n",
      "sub  : (3, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id             model_a     model_b  \\\n",
       "0  30192  gpt-4-1106-preview  gpt-4-0613   \n",
       "1  53567           koala-13b  gpt-4-0613   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>[\"I have three oranges today, I ate an orange ...</td>\n",
       "      <td>[\"You have two oranges today.\"]</td>\n",
       "      <td>[\"You still have three oranges. Eating an oran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>[\"You are a mediator in a heated political deb...</td>\n",
       "      <td>[\"Thank you for sharing the details of the sit...</td>\n",
       "      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                             prompt  \\\n",
       "0  136060  [\"I have three oranges today, I ate an orange ...   \n",
       "1  211333  [\"You are a mediator in a heated political deb...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                    [\"You have two oranges today.\"]   \n",
       "1  [\"Thank you for sharing the details of the sit...   \n",
       "\n",
       "                                          response_b  \n",
       "0  [\"You still have three oranges. Eating an oran...  \n",
       "1  [\"Mr Reddy and Ms Blue both have valid points ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  winner_model_a  winner_model_b  winner_tie\n",
       "0  136060        0.333333        0.333333    0.333333\n",
       "1  211333        0.333333        0.333333    0.333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_DIR = \"/kaggle/input/llm-classification-finetuning/\"\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "sub   = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "print(\"train:\", train.shape)\n",
    "print(\"test :\", test.shape)\n",
    "print(\"sub  :\", sub.shape)\n",
    "\n",
    "display(train.head(2))\n",
    "display(test.head(2))\n",
    "display(sub.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:27:50.911083Z",
     "iopub.status.busy": "2026-02-11T03:27:50.910692Z",
     "iopub.status.idle": "2026-02-11T03:27:50.935403Z",
     "shell.execute_reply": "2026-02-11T03:27:50.934446Z",
     "shell.execute_reply.started": "2026-02-11T03:27:50.911041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20064\n",
       "1    19652\n",
       "2    17761\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_COLS = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "\n",
    "# Convert one-hot targets to a single class label:\n",
    "# 0 = A wins, 1 = B wins, 2 = tie\n",
    "row_sums = train[TARGET_COLS].sum(axis=1)\n",
    "assert (row_sums == 1).all(), f\"Targets are not one-hot in {(row_sums != 1).sum()} rows.\"\n",
    "\n",
    "y = train[TARGET_COLS].values.argmax(axis=1)\n",
    "pd.Series(y).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:27:59.992581Z",
     "iopub.status.busy": "2026-02-11T03:27:59.992234Z",
     "iopub.status.idle": "2026-02-11T03:28:04.598062Z",
     "shell.execute_reply": "2026-02-11T03:28:04.597191Z",
     "shell.execute_reply.started": "2026-02-11T03:27:59.992550Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Is it morally right to try to have a certain percentage of females on managerial positions?\\nOK, does pineapple belong on a pizza? Relax and give me fun answer.\\nThe question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue th',\n",
       " \"Is it morally right to try to have a certain percentage of females on managerial positions?\\nOK, does pineapple belong on a pizza? Relax and give me fun answer.\\nAs an AI, I don't have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a\")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_text(x) -> str:\n",
    "    \"\"\"Normalize prompt/response fields that may be stored as list-like strings.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    if isinstance(x, list):\n",
    "        return \"\\n\".join([str(t) for t in x if t is not None])\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            try:\n",
    "                v = ast.literal_eval(s)\n",
    "                if isinstance(v, list):\n",
    "                    return \"\\n\".join([str(t) for t in v if t is not None])\n",
    "            except Exception:\n",
    "                # If parsing fails, fall back to raw string\n",
    "                return x\n",
    "    return str(x)\n",
    "\n",
    "for col in [\"prompt\", \"response_a\", \"response_b\"]:\n",
    "    train[col] = train[col].apply(normalize_text)\n",
    "    test[col]  = test[col].apply(normalize_text)\n",
    "\n",
    "# Paired texts (prompt + candidate response)\n",
    "train_text_a = (train[\"prompt\"] + \"\\n\" + train[\"response_a\"]).astype(str)\n",
    "train_text_b = (train[\"prompt\"] + \"\\n\" + train[\"response_b\"]).astype(str)\n",
    "\n",
    "test_text_a = (test[\"prompt\"] + \"\\n\" + test[\"response_a\"]).astype(str)\n",
    "test_text_b = (test[\"prompt\"] + \"\\n\" + test[\"response_b\"]).astype(str)\n",
    "\n",
    "train_text_a.iloc[0][:300], train_text_b.iloc[0][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:28:20.059807Z",
     "iopub.status.busy": "2026-02-11T03:28:20.059447Z",
     "iopub.status.idle": "2026-02-11T03:29:32.601794Z",
     "shell.execute_reply": "2026-02-11T03:29:32.600737Z",
     "shell.execute_reply.started": "2026-02-11T03:28:20.059779Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_concat: (57477, 240000)\n",
      "Xt_concat: (3, 240000)\n"
     ]
    }
   ],
   "source": [
    "# Baseline 1: Concatenation features (A | B)\n",
    "# We encode (prompt+response_a) and (prompt+response_b) separately, then concatenate vectors.\n",
    "vec_concat = TfidfVectorizer(max_features=120000, ngram_range=(1, 2), min_df=2)\n",
    "\n",
    "Xa = vec_concat.fit_transform(train_text_a)\n",
    "Xb = vec_concat.transform(train_text_b)\n",
    "X_concat = hstack([Xa, Xb]).tocsr()\n",
    "\n",
    "Xta = vec_concat.transform(test_text_a)\n",
    "Xtb = vec_concat.transform(test_text_b)\n",
    "Xt_concat = hstack([Xta, Xtb]).tocsr()\n",
    "\n",
    "print(\"X_concat:\", X_concat.shape)\n",
    "print(\"Xt_concat:\", Xt_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:29:32.604756Z",
     "iopub.status.busy": "2026-02-11T03:29:32.603621Z",
     "iopub.status.idle": "2026-02-11T03:30:45.035339Z",
     "shell.execute_reply": "2026-02-11T03:30:45.034368Z",
     "shell.execute_reply.started": "2026-02-11T03:29:32.604671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_pair: (57477, 120000)\n",
      "Xt_pair: (3, 120000)\n"
     ]
    }
   ],
   "source": [
    "# Baseline 2: Pairwise difference (A - B)\n",
    "# This directly models relative evidence between A and B.\n",
    "vec_pair = TfidfVectorizer(max_features=120000, ngram_range=(1, 2), min_df=2)\n",
    "\n",
    "Xa2 = vec_pair.fit_transform(train_text_a)\n",
    "Xb2 = vec_pair.transform(train_text_b)\n",
    "X_pair = (Xa2 - Xb2).tocsr()\n",
    "\n",
    "Xta2 = vec_pair.transform(test_text_a)\n",
    "Xtb2 = vec_pair.transform(test_text_b)\n",
    "Xt_pair = (Xta2 - Xtb2).tocsr()\n",
    "\n",
    "print(\"X_pair:\", X_pair.shape)\n",
    "print(\"Xt_pair:\", Xt_pair.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:30:45.036806Z",
     "iopub.status.busy": "2026-02-11T03:30:45.036399Z",
     "iopub.status.idle": "2026-02-11T03:32:39.194866Z",
     "shell.execute_reply": "2026-02-11T03:32:39.193870Z",
     "shell.execute_reply.started": "2026-02-11T03:30:45.036779Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation logloss (concat): 1.0840148499868978\n",
      "Validation logloss (pairwise): 1.0658962371539014\n"
     ]
    }
   ],
   "source": [
    "def train_val_logloss(X, y, test_size=0.15, seed=42):\n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "    clf = LogisticRegression(max_iter=2000, n_jobs=-1, solver=\"lbfgs\")\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    va_proba = clf.predict_proba(X_va)\n",
    "    return log_loss(y_va, va_proba), clf\n",
    "\n",
    "ll_concat, _ = train_val_logloss(X_concat, y)\n",
    "ll_pair, _   = train_val_logloss(X_pair, y)\n",
    "\n",
    "print(\"Validation logloss (concat):\", ll_concat)\n",
    "print(\"Validation logloss (pairwise):\", ll_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:32:39.197313Z",
     "iopub.status.busy": "2026-02-11T03:32:39.197012Z",
     "iopub.status.idle": "2026-02-11T03:32:39.408474Z",
     "shell.execute_reply": "2026-02-11T03:32:39.407458Z",
     "shell.execute_reply.started": "2026-02-11T03:32:39.197277Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>winner</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(-51146.001, -562.0]</th>\n",
       "      <td>0.233739</td>\n",
       "      <td>0.524696</td>\n",
       "      <td>0.241565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-562.0, -108.0]</th>\n",
       "      <td>0.293291</td>\n",
       "      <td>0.377571</td>\n",
       "      <td>0.329138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-108.0, 107.0]</th>\n",
       "      <td>0.298884</td>\n",
       "      <td>0.296616</td>\n",
       "      <td>0.404500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(107.0, 551.0]</th>\n",
       "      <td>0.395013</td>\n",
       "      <td>0.282773</td>\n",
       "      <td>0.322214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(551.0, 42783.0]</th>\n",
       "      <td>0.524697</td>\n",
       "      <td>0.227546</td>\n",
       "      <td>0.247757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "winner                       0         1         2\n",
       "bin                                               \n",
       "(-51146.001, -562.0]  0.233739  0.524696  0.241565\n",
       "(-562.0, -108.0]      0.293291  0.377571  0.329138\n",
       "(-108.0, 107.0]       0.298884  0.296616  0.404500\n",
       "(107.0, 551.0]        0.395013  0.282773  0.322214\n",
       "(551.0, 42783.0]      0.524697  0.227546  0.247757"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verbosity bias check: does the longer response tend to win?\n",
    "len_a = train[\"response_a\"].astype(str).str.len()\n",
    "len_b = train[\"response_b\"].astype(str).str.len()\n",
    "len_diff = len_a - len_b  # >0 means A is longer\n",
    "\n",
    "bias_df = pd.DataFrame({\"len_diff\": len_diff, \"winner\": y})\n",
    "bias_df[\"bin\"] = pd.qcut(bias_df[\"len_diff\"], q=5, duplicates=\"drop\")\n",
    "\n",
    "pd.crosstab(bias_df[\"bin\"], bias_df[\"winner\"], normalize=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:32:39.409938Z",
     "iopub.status.busy": "2026-02-11T03:32:39.409589Z",
     "iopub.status.idle": "2026-02-11T03:42:49.705057Z",
     "shell.execute_reply": "2026-02-11T03:42:49.703367Z",
     "shell.execute_reply.started": "2026-02-11T03:32:39.409904Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation logloss (pairwise + length_diff): 1.0719313718454075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Add a simple bias-aware feature: length difference (A_len - B_len)\n",
    "len_diff_sparse = csr_matrix(len_diff.values.reshape(-1, 1))\n",
    "X_pair_len = hstack([X_pair, len_diff_sparse]).tocsr()\n",
    "\n",
    "ll_pair_len, clf_pair_len_val = train_val_logloss(X_pair_len, y)\n",
    "print(\"Validation logloss (pairwise + length_diff):\", ll_pair_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ablation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:42:49.708159Z",
     "iopub.status.busy": "2026-02-11T03:42:49.707758Z",
     "iopub.status.idle": "2026-02-11T03:42:49.729612Z",
     "shell.execute_reply": "2026-02-11T03:42:49.728550Z",
     "shell.execute_reply.started": "2026-02-11T03:42:49.708128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Validation LogLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pairwise TF-IDF (A-B)</td>\n",
       "      <td>1.065896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pairwise TF-IDF + length_diff</td>\n",
       "      <td>1.071931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF concat (A|B)</td>\n",
       "      <td>1.084015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model  Validation LogLoss\n",
       "1          Pairwise TF-IDF (A-B)            1.065896\n",
       "2  Pairwise TF-IDF + length_diff            1.071931\n",
       "0            TF-IDF concat (A|B)            1.084015"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablation = pd.DataFrame([\n",
    "    {\"Model\": \"TF-IDF concat (A|B)\", \"Validation LogLoss\": ll_concat},\n",
    "    {\"Model\": \"Pairwise TF-IDF (A-B)\", \"Validation LogLoss\": ll_pair},\n",
    "    {\"Model\": \"Pairwise TF-IDF + length_diff\", \"Validation LogLoss\": ll_pair_len},\n",
    "]).sort_values(\"Validation LogLoss\")\n",
    "\n",
    "ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:42:49.732707Z",
     "iopub.status.busy": "2026-02-11T03:42:49.732317Z",
     "iopub.status.idle": "2026-02-11T03:54:27.270528Z",
     "shell.execute_reply": "2026-02-11T03:54:27.269369Z",
     "shell.execute_reply.started": "2026-02-11T03:42:49.732655Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: submission_best.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.181359</td>\n",
       "      <td>0.473148</td>\n",
       "      <td>0.345493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.354204</td>\n",
       "      <td>0.403292</td>\n",
       "      <td>0.242504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.223820</td>\n",
       "      <td>0.298961</td>\n",
       "      <td>0.477219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "0   136060        0.181359        0.473148    0.345493\n",
       "1   211333        0.354204        0.403292    0.242504\n",
       "2  1233961        0.223820        0.298961    0.477219"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train improved model on full training set (pairwise + length_diff)\n",
    "# Note: lbfgs may show a convergence warning at this iteration budget, but will still produce valid predictions.\n",
    "clf_pair_len = LogisticRegression(max_iter=2000, n_jobs=-1, solver=\"lbfgs\")\n",
    "clf_pair_len.fit(X_pair_len, y)\n",
    "\n",
    "# Build test features\n",
    "test_len_a = test[\"response_a\"].astype(str).str.len().values\n",
    "test_len_b = test[\"response_b\"].astype(str).str.len().values\n",
    "test_len_diff_sparse = csr_matrix((test_len_a - test_len_b).reshape(-1, 1))\n",
    "\n",
    "Xt_pair_len = hstack([Xt_pair, test_len_diff_sparse]).tocsr()\n",
    "\n",
    "te_proba = clf_pair_len.predict_proba(Xt_pair_len)\n",
    "\n",
    "sub[\"winner_model_a\"] = te_proba[:, 0]\n",
    "sub[\"winner_model_b\"] = te_proba[:, 1]\n",
    "sub[\"winner_tie\"]     = te_proba[:, 2]\n",
    "\n",
    "out_path = \"submission_best.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "print(\"Wrote:\", out_path)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- `model_a` / `model_b` are present in `train.csv` but not in `test.csv`. This notebook **does not use** them as features to avoid train-only leakage.\n",
    "- Next extensions (optional): transformer embeddings for pairwise scoring, bias-corrected objectives, or a compact reward model fine-tuning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Pairwise preference modeling significantly improved performance compared to the concatenation baseline.\n",
    "- Analysis revealed a measurable verbosity bias, where longer responses tend to be selected more frequently.\n",
    "- Incorporating a simple length-difference feature provided additional performance gains, demonstrating the effectiveness of bias-aware feature engineering.\n",
    "- This baseline establishes a reproducible starting point for future extensions such as transformer-based reward modeling or reasoning-aware fine-tuning.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "sourceId": 86518,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
